{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONIOENCODING'] = 'latin-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_io_encoding = ! echo $PYTHONIOENCODING\n",
    "assert_equal(python_io_encoding.s, 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if not(line.startswith('Year')):\n",
    "        aline = line.split(\",\")\n",
    "        print(aline[16]+'\\t'+aline[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! chmod u+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t60\n",
      "BWI\t64\n",
      "BWI\t80\n",
      "BWI\t66\n",
      "BWI\t62\n",
      "BWI\t61\n",
      "BWI\t61\n",
      "BWI\t60\n",
      "BWI\t52\n",
      "BWI\t62\n",
      "BWI\t62\n",
      "BWI\t55\n",
      "BWI\t60\n",
      "BWI\t61\n",
      "BWI\t63\n",
      "PHL\t53\n",
      "PHL\t54\n",
      "PHL\t55\n",
      "PHL\t53\n",
      "PHL\t50\n",
      "PHL\tNA\n",
      "PHL\t57\n",
      "PHL\t48\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t49\n",
      "PHL\t75\n",
      "PHL\t49\n",
      "PHL\t50\n",
      "PHL\t49\n",
      "PHL\tNA\n",
      "PHL\t46\n",
      "PHL\tNA\n",
      "PHL\t51\n",
      "PHL\t53\n",
      "PHL\t52\n",
      "PHL\t52\n",
      "PHL\t54\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t51\n",
      "PHL\t49\n",
      "PHL\t49\n",
      "CLT\t82\n",
      "CLT\t82\n",
      "CLT\t78\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 2001.csv > 2001.csv.head\n",
    "map_out_head = ! ./mapper.py < 2001.csv.head\n",
    "print('\\n'.join(map_out_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(\n",
    "    map_out_head,\n",
    "    ['BWI\\t60','BWI\\t64','BWI\\t80','BWI\\t66','BWI\\t62','BWI\\t61',\n",
    "     'BWI\\t61','BWI\\t60','BWI\\t52','BWI\\t62','BWI\\t62','BWI\\t55',\n",
    "     'BWI\\t60','BWI\\t61','BWI\\t63','PHL\\t53','PHL\\t54','PHL\\t55',\n",
    "     'PHL\\t53','PHL\\t50','PHL\\tNA','PHL\\t57','PHL\\t48','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t49','PHL\\t75',\n",
    "     'PHL\\t49','PHL\\t50','PHL\\t49','PHL\\tNA','PHL\\t46','PHL\\tNA',\n",
    "     'PHL\\t51','PHL\\t53','PHL\\t52','PHL\\t52','PHL\\t54','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t51','PHL\\t49','PHL\\t49','CLT\\t82','CLT\\t82',\n",
    "     'CLT\\t78']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "sep = '\\t'\n",
    "\n",
    "minmax = dict()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    airport, airtime = line.split('\\t', 1)\n",
    "    \n",
    "    if airtime!='NA':\n",
    "        at = int(airtime)\n",
    "   \n",
    "    if airport not in minmax.keys():\n",
    "        minmax[airport] = [float(\"inf\"),-float(\"inf\")]\n",
    "    \n",
    "    if at>minmax[airport][1]:\n",
    "        minmax[airport][1]=at\n",
    "                \n",
    "    if at<minmax[airport][0]:\n",
    "        minmax[airport][0]=at\n",
    "\n",
    "for i in sorted(minmax):\n",
    "    print(i+'\\t'+str(minmax[i][0])+'\\t'+str(minmax[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t52\t80\n",
      "CLT\t78\t82\n",
      "PHL\t46\t75\n"
     ]
    }
   ],
   "source": [
    "red_head_out = ! ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(red_head_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t16\t180\n",
      "ABI\t28\t85\n",
      "ABQ\t15\t264\n",
      "ACT\t19\t81\n",
      "ACY\t33\t33\n",
      "ADQ\t32\t67\n",
      "AKN\t12\t54\n",
      "ALB\t23\t360\n",
      "AMA\t30\t130\n",
      "ANC\t23\t428\n"
     ]
    }
   ],
   "source": [
    "mapred_out = ! ./mapper.py < 2001.csv | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(mapred_out[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(len(mapred_out), 231)\n",
    "assert_equal(mapred_out[:5], ['ABE\\t16\\t180', 'ABI\\t28\\t85', 'ABQ\\t15\\t264', 'ACT\\t19\\t81', 'ACY\\t33\\t33'])\n",
    "assert_equal(mapred_out[-5:], ['TYS\\t11\\t177', 'VPS\\t28\\t123', 'WRG\\t5\\t38', 'XNA\\t33\\t195', 'YAK\\t28\\t72'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS: Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 22:28:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "17/08/18 22:28:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_HOME/sbin/stop-dfs.sh\n",
    "! $HADOOP_HOME/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-cdfdc43e-b89a-420f-9f8d-3c75fb28f2b1\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Y\" | $HADOOP_HOME/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /usr/local/Cellar/hadoop/2.8.0/etc/hadoop/hadoop-env.sh: No such file or directory\n",
      "17/08/18 22:29:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/hadoop-Constance-namenode-host-173-230-8-252.ilcuarb.clients.pavlovmedia.com.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/hadoop-Constance-datanode-host-173-230-8-252.ilcuarb.clients.pavlovmedia.com.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/hadoop-Constance-secondarynamenode-host-173-230-8-252.ilcuarb.clients.pavlovmedia.com.out\n",
      "17/08/18 22:29:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/yarn-Constance-resourcemanager-host-173-230-8-252.ilcuarb.clients.pavlovmedia.com.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/yarn-Constance-nodemanager-host-173-230-8-252.ilcuarb.clients.pavlovmedia.com.out\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/etc/hadoop-env.sh\n",
    "!$HADOOP_HOME/sbin/start-dfs.sh\n",
    "!$HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 22:30:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Safe mode is OFF\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_HOME/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS: Create directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -rm -r -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 22:46:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/08/18 22:46:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /user\n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /user/data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 22:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - Constance supergroup          0 2017-08-18 22:46 /user/data_scientist\n"
     ]
    }
   ],
   "source": [
    "ls_user = ! $HADOOP_HOME/bin/hdfs dfs -ls /user/\n",
    "print('\\n'.join(ls_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_true('/user/data_scientist' in ls_user.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:14:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/user/data_scientist/wc': File exists\n",
      "17/08/18 23:15:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/user/data_scientist/wc/in': File exists\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /user/data_scientist/wc\n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:21:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - Constance supergroup          0 2017-08-18 22:48 /user/data_scientist/wc/in\n"
     ]
    }
   ],
   "source": [
    "ls_wc = !$HADOOP_HOME/bin/hdfs dfs -ls /user/data_scientist/wc\n",
    "print('\\n'.join(ls_wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS: Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -put 2001.csv /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:25:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   1 Constance supergroup  600411462 2017-08-18 23:21 /user/data_scientist/wc/in/2001.csv\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -ls /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\n",
      "packageJobJar: [/var/folders/kg/sql8zr092nz03ztkw2prx_z80000gn/T/hadoop-unjar7640413220499755904/] [] /var/folders/kg/sql8zr092nz03ztkw2prx_z80000gn/T/streamjob2576565045769441957.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/08/18 23:36:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/08/18 23:36:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/08/18 23:36:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/08/18 23:36:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/08/18 23:36:24 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "17/08/18 23:36:25 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/08/18 23:36:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1503113388432_0002\n",
      "17/08/18 23:36:26 INFO impl.YarnClientImpl: Submitted application application_1503113388432_0002\n",
      "17/08/18 23:36:26 INFO mapreduce.Job: The url to track the job: http://host-173-230-8-252.ilcuarb.clients.pavlovmedia.com:8088/proxy/application_1503113388432_0002/\n",
      "17/08/18 23:36:26 INFO mapreduce.Job: Running job: job_1503113388432_0002\n",
      "17/08/18 23:37:33 INFO mapreduce.Job: Job job_1503113388432_0002 running in uber mode : false\n",
      "17/08/18 23:37:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/08/18 23:38:00 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "17/08/18 23:38:07 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "17/08/18 23:38:11 INFO mapreduce.Job:  map 93% reduce 0%\n",
      "17/08/18 23:38:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/08/18 23:38:26 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "17/08/18 23:38:32 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "17/08/18 23:38:38 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "17/08/18 23:38:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/08/18 23:38:42 INFO mapreduce.Job: Job job_1503113388432_0002 completed successfully\n",
      "17/08/18 23:38:42 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=56042387\n",
      "\t\tFILE: Number of bytes written=112922499\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=600428386\n",
      "\t\tHDFS: Number of bytes written=2464\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=172729\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=37682\n",
      "\t\tTotal time spent by all map tasks (ms)=172729\n",
      "\t\tTotal time spent by all reduce tasks (ms)=37682\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=172729\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=37682\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=176874496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=38586368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5967781\n",
      "\t\tMap output records=5967780\n",
      "\t\tMap output bytes=44106821\n",
      "\t\tMap output materialized bytes=56042411\n",
      "\t\tInput split bytes=540\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=231\n",
      "\t\tReduce shuffle bytes=56042411\n",
      "\t\tReduce input records=5967780\n",
      "\t\tReduce output records=231\n",
      "\t\tSpilled Records=11935560\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=900\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=1148190720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=600427846\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2464\n",
      "17/08/18 23:38:42 INFO streaming.StreamJob: Output directory: wc/out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Delete output directory (if it exists)\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r -f wc/out\n",
    "\n",
    "# Grab current streaming lib jar filename\n",
    "streaming_file=$(ls $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming*)\n",
    "\n",
    "# Run the Map Reduce task within Hadoop\n",
    "$HADOOP_HOME/bin/hadoop jar $streaming_file \\\n",
    "    -files mapper.py,reducer.py -input /user/data_scientist/wc/in \\\n",
    "    -output wc/out -mapper \"python mapper.py\" -reducer \"python reducer.py\"\\\n",
    "    -cmdenv PYTHONIOENCODING='latin-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:55:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 Constance supergroup          0 2017-08-18 23:38 wc/out/_SUCCESS\n",
      "-rw-r--r--   1 Constance supergroup       2464 2017-08-18 23:38 wc/out/part-00000\n"
     ]
    }
   ],
   "source": [
    "ls_wc_out = !$HADOOP_HOME/bin/hdfs dfs -ls wc/out\n",
    "print('\\n'.join(ls_wc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_true('wc/out/_SUCCESS' in ls_wc_out.s)\n",
    "assert_true('wc/out/part-00000' in ls_wc_out.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/18 23:39:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ABE\t16\t180\n",
      "ABI\t28\t85\n",
      "ABQ\t15\t264\n",
      "ACT\t19\t81\n",
      "ACY\t33\t33\n",
      "ADQ\t32\t67\n",
      "AKN\t12\t54\n",
      "ALB\t23\t360\n",
      "AMA\t30\t130\n"
     ]
    }
   ],
   "source": [
    "stream_out = ! $HADOOP_HOME/bin/hdfs dfs -cat wc/out/part-00000\n",
    "print('\\n'.join(stream_out[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/08/19 00:00:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted wc/out\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -rm -r -f -skipTrash wc/out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
