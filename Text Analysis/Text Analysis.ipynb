{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true,\n",
    "    assert_false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, fileID):\n",
    "    '''\n",
    "    Tokenizes the, casting all words to lower case, stripping out punctuation marks, spaces,\n",
    "    and words not made of one or more alphanumerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus\n",
    "    fileID: A string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: a list of strings\n",
    "    '''\n",
    "    \n",
    "    words=[word.lower() for word in corpus.words(fileID)]\n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    words= [i for i in words if not pattern.match(i)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monty = tokenize(webtext, 'grail.txt')\n",
    "assert_is_instance(monty, list)\n",
    "assert_equal(len(monty), 11602)\n",
    "\n",
    "assert_true(all(isinstance(w, str) for w in monty))\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in monty))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in monty))\n",
    "\n",
    "assert_equal(monty[8:13], ['whoa', 'there', 'clop', 'clop', 'clop'])\n",
    "assert_equal(monty[20:45], ['it', 'is', 'i', 'arthur', 'son', 'of', 'uther', 'pendragon',\\\n",
    "                            'from', 'the', 'castle', 'of', 'camelot', 'king', 'of', 'the',\\\n",
    "                            'britons', 'defeator', 'of', 'the', 'saxons', 'sovereign', 'of', 'all', 'england'])\n",
    "\n",
    "pirates= tokenize(webtext, 'pirates.txt')\n",
    "assert_is_instance(pirates, list)\n",
    "assert_equal(len(pirates), 17143)\n",
    "\n",
    "assert_true(all(isinstance(w, str) for w in pirates))\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in pirates))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in pirates))\n",
    "\n",
    "assert_equal(pirates[100:110], ['the', 'left', 'in', 'the', 'barn', 'where', 'the', 'marines', 'enter', 'liz'])\n",
    "assert_equal(pirates[-10:], ['left', 'shoulder', 'faces', 'the', 'camera', 'and', 'snarls', 'scene', 'end', 'credits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(word_ls):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_ls: A list of of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div) called tup\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    \n",
    "    counts = nltk.FreqDist(word_ls)\n",
    "    num_words = len(word_ls)\n",
    "    num_tokens = len(counts)\n",
    "    lex_div  =  num_words / num_tokens\n",
    "    return num_tokens, num_words, lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monty_tokens, monty_words, mld = count_words(monty)\n",
    "assert_is_instance(monty_tokens, int)\n",
    "assert_is_instance(monty_words, int)\n",
    "assert_is_instance(mld, float)\n",
    "assert_equal(monty_tokens, 1823)\n",
    "assert_equal(monty_words, 11602)\n",
    "assert_almost_equal(mld, 6.364234777838727)\n",
    "\n",
    "pirate_tokens, pirate_words, pld = count_words(pirates)\n",
    "assert_is_instance(pirate_tokens, int)\n",
    "assert_is_instance(pirate_words, int)\n",
    "assert_is_instance(pld, float)\n",
    "assert_equal(pirate_tokens, 2731)\n",
    "assert_equal(pirate_words, 17143)\n",
    "assert_almost_equal(pld, 6.277187843280849)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common(words, num_top_words):\n",
    "    '''\n",
    "    Takes the output of tokenize and find the most common words within that list,\n",
    "    returning a list of tuples containing the most common words and their number \n",
    "    of occurances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings\n",
    "    num_top_words:  An int. The number of most common words (and tuples) \n",
    "                    that will be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    top_words:  A list of tuples, where each tuple contains a word and\n",
    "                its number of occurances.\n",
    "    '''\n",
    "    \n",
    "    counts = nltk.FreqDist(words)\n",
    "    result=counts.most_common(num_top_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yarr = most_common(pirates, 5)\n",
    "\n",
    "assert_is_instance(yarr, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in yarr))\n",
    "assert_true(all(isinstance(t, str) for t, f in yarr))\n",
    "assert_true(all(isinstance(f, int) for t, f in yarr))\n",
    "\n",
    "assert_equal(len(most_common(pirates, 10)), 10)\n",
    "assert_equal(len(most_common(pirates, 20)), 20)\n",
    "assert_equal(yarr, [('the', 1073), ('jack', 470), ('a', 434), ('to', 372), ('of', 285)])\n",
    "\n",
    "shrubbery = most_common(monty, 5)\n",
    "assert_is_instance(shrubbery, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in shrubbery))\n",
    "assert_true(all(isinstance(t, str) for t, f in shrubbery))\n",
    "assert_true(all(isinstance(f, int) for t, f in shrubbery))\n",
    "\n",
    "assert_equal(len(most_common(monty, 15)), 15)\n",
    "assert_equal(len(most_common(monty, 37)), 37)\n",
    "assert_equal(shrubbery, [('the', 334), ('you', 265), ('arthur', 261), ('i', 260), ('a', 238)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hapax(words):\n",
    "    '''\n",
    "    Finds all hapaxes from the \"words\" list of strings.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hapax: A list of strings\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    counts = nltk.FreqDist(words)\n",
    "    result=counts.hapaxes()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(hapax(monty), list)\n",
    "assert_true(all(isinstance(w, str) for w in hapax(monty)))\n",
    "assert_equal(len(hapax(monty)), 977)\n",
    "assert_equal(sorted(hapax(monty))[-5:],['zhiv', 'zone', 'zoo', 'zoop', 'zoosh'])\n",
    "\n",
    "assert_is_instance(hapax(pirates), list)\n",
    "assert_true(all(isinstance(w, str) for w in hapax(pirates)))\n",
    "assert_equal(len(hapax(pirates)), 1433)\n",
    "assert_equal(sorted(hapax(pirates))[-5:],['yeah', 'yep', 'yours', 'yourselves', 'zooming'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def long_words(words, length=10):\n",
    "    '''\n",
    "    Finds all words in \"words\" longer than \"length\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An list of strings.\n",
    "    length: An int. Default: 10\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    result = [word for word in words if len(word) > length]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monty_l = long_words(monty, 12)\n",
    "assert_is_instance(monty_l, list)\n",
    "assert_true(all(isinstance(w, str) for w in monty_l))    \n",
    "assert_equal(len(monty_l), 6)\n",
    "assert_equal(\n",
    "    set(monty_l),\n",
    "    set(['unfortunately', 'understanding', 'oooohoohohooo', 'indefatigable', 'camaaaaaargue', 'automatically'])\n",
    "    )\n",
    "assert_equal(len(long_words(monty,10)), 68)\n",
    "assert_equal(len(long_words(monty,11)), 37)\n",
    "\n",
    "\n",
    "pirate_l = long_words(pirates, 13)\n",
    "assert_is_instance(pirate_l, list)\n",
    "assert_true(all(isinstance(w, str) for w in monty_l))    \n",
    "assert_equal(len(pirate_l), 5)\n",
    "assert_equal(\n",
    "    set(pirate_l),\n",
    "    set(['simultanenously', 'responsibility', 'reconciliatory', 'incapacitorially', 'enthusiastically']))\n",
    "assert_equal(len(long_words(pirates,10)), 107)\n",
    "assert_equal(len(long_words(pirates,12)), 29)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
