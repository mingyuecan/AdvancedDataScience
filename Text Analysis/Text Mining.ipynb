{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true\n",
    ")\n",
    "from numpy.testing import assert_array_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_genres(n):\n",
    "    '''\n",
    "    Selects genres with more than n files. Returns raw data and the genre of each file\n",
    "    in the selected genres as two 1d numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: An integer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (raw, genres)\n",
    "    raw: A 1d numpy array.\n",
    "    genres: A 1d numpy array.\n",
    "    '''\n",
    "    \n",
    "    fl=[]\n",
    "    for genere in brown.categories():\n",
    "        length=len(brown.fileids(categories=genere))\n",
    "        if length>n:\n",
    "            fl.append(brown.fileids([genere]))\n",
    "    fid=sum(fl, [])\n",
    "    fid.sort()\n",
    "    raw=np.array([brown.raw(i) for i in fid])\n",
    "    genres=np.array(sum([brown.categories(j) for j in fid],[]))\n",
    "    return raw, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_raw, t1_genres = select_genres(70)\n",
    "assert_equal(np.shape(t1_raw), (155,))\n",
    "assert_equal(np.shape(t1_genres), (155,))\n",
    "assert_array_equal(t1_genres, ['belles_lettres']*75+['learned']*80)\n",
    "assert_equal(t1_raw[5][:50], 'Die/fw-at Frist/fw-nn ist/fw-bez um/fw-rb ,/, und/')\n",
    "assert_equal(t1_raw[120][120:160], 'agricultural/jj areas/nns in/in the/at w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_raw, t1_genres = select_genres(70)\n",
    "assert_equal(np.shape(t1_raw), (155,))\n",
    "assert_equal(np.shape(t1_genres), (155,))\n",
    "assert_array_equal(t1_genres, ['belles_lettres']*75+['learned']*80)\n",
    "assert_equal(t1_raw[5][:50], 'Die/fw-at Frist/fw-nn ist/fw-bez um/fw-rb ,/, und/')\n",
    "assert_equal(t1_raw[120][120:160], 'agricultural/jj areas/nns in/in the/at w')\n",
    "\n",
    "t2_raw, t2_genres = select_genres(29)\n",
    "assert_equal(np.shape(t2_raw), (313,))\n",
    "assert_equal(np.shape(t2_genres), (313,))\n",
    "assert_array_equal(t2_genres, ['news']*44+['hobbies']*36+['lore']*48+['belles_lettres']*75+['government']*30+['learned']*80)\n",
    "assert_equal(t2_raw[300][-80:], \" is/bez not/* generally/rb used/vbn over-hand/rb ,/, but/cc under/rb ''/'' ./.\\n\\n\")\n",
    "assert_equal(t2_raw[249][490:530], 's from/in the/at cortex/nn to/in the/at ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_raw, t_genres = select_genres(27)\n",
    "t_X_train, t_X_test, t_y_train, t_y_test = train_test_split(t_raw, \n",
    "                                                            t_genres, \n",
    "                                                            random_state=check_random_state(0), \n",
    "                                                            test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(X_train, y_train, X_test):\n",
    "    '''\n",
    "    Creates a document term matrix and uses KNC classifier to make document classifications.\n",
    "    Uses unigrams, bigrams, and trigrams.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A 1d numpy array of strings.\n",
    "    y_train: A 1d numpy array of strings.\n",
    "    X_test: A 1d numpy array of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A 1d numpy array.\n",
    "    '''\n",
    "    \n",
    "    tools = [('tf', TfidfVectorizer()), ('knc', KNeighborsClassifier())]\n",
    "    clf = Pipeline(tools)\n",
    "    clf.set_params(tf__stop_words = 'english', \\\n",
    "                tf__ngram_range=(1,3), \\\n",
    "                tf__lowercase=True,\\\n",
    "                tf__min_df=3, \\\n",
    "                tf__max_df=0.7)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNC prediction accuracy =  52.5%\n"
     ]
    }
   ],
   "source": [
    "clf1, y_pred1 = ngram(t_X_train, t_y_train, t_X_test)\n",
    "score1 = accuracy_score(y_pred1, t_y_test)\n",
    "print(\"KNC prediction accuracy = {0:5.1f}%\".format(100.0 * score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf1, Pipeline)\n",
    "assert_is_instance(y_pred1, np.ndarray)\n",
    "tf1 = clf1.named_steps['tf']\n",
    "assert_is_instance(tf1, TfidfVectorizer)\n",
    "assert_is_instance(clf1.named_steps['knc'], KNeighborsClassifier)\n",
    "assert_equal(tf1.stop_words, 'english')\n",
    "assert_equal(tf1.ngram_range, (1, 3))\n",
    "assert_equal(tf1.min_df, 3)\n",
    "assert_equal(tf1.max_df, 0.7)\n",
    "assert_equal(len(y_pred1), len(t_y_test))\n",
    "assert_array_equal(y_pred1[:5], ['belles_lettres', 'government', 'romance', 'belles_lettres', 'government'])\n",
    "assert_array_equal(y_pred1[-5:], ['government', 'lore', 'government', 'learned', 'adventure'])\n",
    "assert_almost_equal(score1, 0.52500000000000002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Converts text into tokens. Same function as in the \"introduction to text mining\" notebook.\n",
    "    Uses Snowball Stemmer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: a string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens: a map object.\n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = EnglishStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(X_train, y_train, X_test):\n",
    "    '''\n",
    "    Creates a document term matrix and uses KNC classifier to make document classifications.\n",
    "    Uses the Snowball stemmer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A 1d numpy array of strings.\n",
    "    y_train: A 1d numpy array of strings.\n",
    "    X_test: A 1d numpy array of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A 1d numpy array.\n",
    "    '''\n",
    "    \n",
    "    tools = [('tf', TfidfVectorizer()), ('knc', KNeighborsClassifier())]\n",
    "    clf = Pipeline(tools)\n",
    "    clf.set_params(tf__stop_words = 'english', \\\n",
    "                tf__ngram_range=(1,3), \\\n",
    "                tf__lowercase=True,\\\n",
    "                tf__min_df=3, \\\n",
    "                tf__max_df=0.7,\\\n",
    "                tf__tokenizer=tokenize)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNC prediction accuracy =  42.0%\n"
     ]
    }
   ],
   "source": [
    "clf2, y_pred2 = stem(t_X_train[:100], t_y_train[:100], t_X_test[:50])\n",
    "score2 = accuracy_score(y_pred2, t_y_test[:50])\n",
    "print(\"KNC prediction accuracy = {0:5.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf2, Pipeline)\n",
    "assert_is_instance(y_pred2, np.ndarray)\n",
    "tf2 = clf2.named_steps['tf']\n",
    "assert_is_instance(tf2, TfidfVectorizer)\n",
    "assert_is_instance(clf2.named_steps['knc'], KNeighborsClassifier)\n",
    "assert_equal(tf2.stop_words, 'english')\n",
    "assert_equal(tf2.ngram_range, (1, 3))\n",
    "assert_equal(tf2.min_df, 3)\n",
    "assert_equal(tf2.max_df, 0.7)\n",
    "\n",
    "assert_equal(len(y_pred2), 50)\n",
    "assert_array_equal(y_pred2[:5], ['lore', 'learned', 'romance', 'belles_lettres', 'learned'])\n",
    "assert_array_equal(y_pred2[-5:], ['fiction', 'romance', 'belles_lettres', 'romance', 'learned'])\n",
    "assert_almost_equal(score2, 0.41999999999999998 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_tokens(X_train, y_train, X_test, random_state, k, n):\n",
    "    '''\n",
    "    First, applies clustering analysis to a feature matrix.\n",
    "    Then, identifies the most frequently used words in \"icluster\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A 1d numpy array of strings.\n",
    "    y_train: A 1d numpy array of strings.\n",
    "    X_test: A 1d numpy array of strings.\n",
    "    random_state: A np.random.RandomState instance for KMeans.\n",
    "    k: An int. The number of clusters.\n",
    "    n: An int. Specifies how many tokens for each cluster should be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clf: A Pipeline instance.\n",
    "    tokens: A 2d numpy array of strings with shape of (n_clusters, n_tokens of each cluster)\n",
    "    '''\n",
    "    \n",
    "    tools = [('tf', TfidfVectorizer()), ('km', KMeans(n_clusters=k,random_state=random_state))]\n",
    "    clf = Pipeline(tools)\n",
    "    clf.set_params(tf__stop_words = 'english', \\\n",
    "                tf__ngram_range=(1,1), \\\n",
    "                tf__lowercase=True,\\\n",
    "                tf__min_df=3, \\\n",
    "                tf__max_df=0.7)\n",
    "    clf.fit(X_train)\n",
    "    order_centroids = clf.named_steps['km'].cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = clf.named_steps['tf'].get_feature_names()\n",
    "    atoken=[]\n",
    "    for idx in range(k):\n",
    "        for jdx in order_centroids[idx, :n]:\n",
    "            atk=terms[jdx]\n",
    "            atoken.append(atk)\n",
    "    tokens=np.reshape(atoken, (k, n))\n",
    "    return clf, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tokens per cluster:\n",
      "---------------------------------------------\n",
      "Cluster 0: fw nil bridge pont nps\n",
      "Cluster 1: men man said eyes dod\n",
      "Cluster 2: hl costs shelter foam foods\n",
      "Cluster 3: college mrs students school education\n",
      "Cluster 4: said dod uh ll bem\n",
      "Cluster 5: hl nps state president law\n",
      "Cluster 6: hl year tax sales 1960\n",
      "Cluster 7: af hl temperature pressure fig\n",
      "Cluster 8: nc fw man human experience\n"
     ]
    }
   ],
   "source": [
    "k3 = len(np.unique(t_genres))\n",
    "n3 = 5\n",
    "clf3, tokens3 = get_top_tokens(t_X_train, t_y_train, t_X_test, check_random_state(0), k3, n3)\n",
    "print('Top {} tokens per cluster:'.format(n3))\n",
    "print('-'*45)\n",
    "for i in range(k3):\n",
    "    print(\"Cluster {0}: {1}\".format(i, ' '.join(tokens3[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf3, Pipeline)\n",
    "tf3 = clf3.named_steps['tf']\n",
    "assert_is_instance(tf3, TfidfVectorizer)\n",
    "km3 = clf3.named_steps['km']\n",
    "assert_is_instance(km3, KMeans)\n",
    "assert_equal(tf3.stop_words, 'english')\n",
    "assert_equal(tf3.ngram_range, (1, 1))\n",
    "assert_equal(tf3.min_df, 3)\n",
    "assert_equal(tf3.max_df, 0.7)\n",
    "assert_equal(km3.n_clusters, k3)\n",
    "assert_equal(np.shape(tokens3), (9, 5))\n",
    "assert_array_equal(tokens3, [['fw', 'nil', 'bridge', 'pont', 'nps'],\n",
    "                             ['men', 'man', 'said', 'eyes', 'dod'],\n",
    "                             ['hl', 'costs', 'shelter', 'foam', 'foods'],\n",
    "                             ['college', 'mrs', 'students', 'school', 'education'],\n",
    "                             ['said', 'dod', 'uh', 'll', 'bem'],\n",
    "                             ['hl', 'nps', 'state', 'president', 'law'],\n",
    "                             ['hl', 'year', 'tax', 'sales', '1960'],\n",
    "                             ['af', 'hl', 'temperature', 'pressure', 'fig'],\n",
    "                             ['nc', 'fw', 'man', 'human', 'experience']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf4, tokens4 = get_top_tokens(t_X_train, t_y_train, t_X_test, check_random_state(0), k3, 3)\n",
    "assert_array_equal(tokens4[0], ['fw', 'nil', 'bridge'])\n",
    "assert_array_equal(tokens4[6], ['hl', 'year', 'tax'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
