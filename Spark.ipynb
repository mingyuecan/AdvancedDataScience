{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14.1. Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile('ml-latest-small/ratings.csv')\n",
    "\n",
    "assert_is_instance(text_file, pyspark.rdd.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ratings_csv(rdd):\n",
    "    '''\n",
    "    Creates an RDD by transforming `ratings.csv`\n",
    "    into columns with appropriate data types.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    data = rdd.map(lambda l: l.split(\",\")) \\\n",
    "             .map(lambda p: (p[0], p[1], p[2], p[3])) \\\n",
    "             .filter(lambda line: 'userId' not in line)\n",
    "    cols = data.filter(lambda line: 'NA' not in line)\n",
    "    result=cols.map(lambda p: (int(p[0]), int(p[1]), float(p[2]), int(p[3])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 31, 2.5, 1260759144), (1, 1029, 3.0, 1260759179), (1, 1061, 3.0, 1260759182)]\n"
     ]
    }
   ],
   "source": [
    "ratings = read_ratings_csv(text_file)\n",
    "print(ratings.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(ratings, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(ratings.count(), 100004)\n",
    "assert_equal(len(ratings.first()), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(rdd):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    result=rdd.filter(lambda p: p[2]>3)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favorable = filter_favorable_ratings(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(favorable, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(favorable.count(), 62106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(rdd, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An int.\n",
    "    '''\n",
    "    \n",
    "    n_reviews=rdd.filter(lambda p: p[1]==movie_id).count()\n",
    "    \n",
    "    return n_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(n_toy_story, int)\n",
    "\n",
    "test = [find_n_reviews(favorable, n) for n in range(5)]\n",
    "assert_equal(test, [0, 182, 51, 24, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14.2. Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance\n",
    "from pandas.util.testing import assert_frame_equal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = 'ml-latest-small/ratings.csv'\n",
    "text_file = sc.textFile(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df(rdd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "    '''\n",
    "    new=read_ratings_csv(rdd)\n",
    "    sqlContext = SQLContext(sc)\n",
    "    schemaString = \"userId movieId rating timestamp\"\n",
    "    fieldTypes = [IntegerType(), IntegerType(), FloatType(), IntegerType()]\n",
    "    f_data = [StructField(field_name, field_type, True) \\\n",
    "              for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "    schema = StructType(f_data)\n",
    "    df = sqlContext.createDataFrame(new, schema)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = create_df(text_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(df, pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "# convert the Spark dataframe to Pandas dataframe\n",
    "df_pd = pd.read_csv(csv_path)\n",
    "assert_frame_equal(df.toPandas(), df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(df):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    result=df.filter(df['rating'] >3).select(df['movieId'], df['rating'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|rating|\n",
      "+-------+------+\n",
      "|   1172|   4.0|\n",
      "|   1339|   3.5|\n",
      "|   1953|   4.0|\n",
      "|   2105|   4.0|\n",
      "|     10|   4.0|\n",
      "|     17|   5.0|\n",
      "|     39|   5.0|\n",
      "|     47|   4.0|\n",
      "|     50|   4.0|\n",
      "|    110|   4.0|\n",
      "|    150|   5.0|\n",
      "|    153|   4.0|\n",
      "|    222|   5.0|\n",
      "|    253|   4.0|\n",
      "|    261|   4.0|\n",
      "|    265|   5.0|\n",
      "|    266|   5.0|\n",
      "|    273|   4.0|\n",
      "|    296|   4.0|\n",
      "|    314|   4.0|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "favorable = filter_favorable_ratings(df)\n",
    "favorable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(favorable, pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "favorable_pd = df_pd.loc[df_pd['rating'] > 3.0, ['movieId', 'rating']].reset_index(drop=True)\n",
    "assert_frame_equal(favorable.toPandas(), favorable_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(df, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    n_reviews: An int.\n",
    "    '''\n",
    "    \n",
    "    n_reviews=df.filter(df['movieId'] == movie_id).count()\n",
    "    \n",
    "    return n_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(n_toy_story, int)\n",
    "\n",
    "test = [find_n_reviews(favorable, n) for n in range(1, 6)]\n",
    "test_pd = favorable_pd.groupby('movieId').size()[:5].tolist()\n",
    "assert_equal(test, test_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14.3. Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal, assert_is_instance,\n",
    "    assert_true, assert_almost_equal\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile('2001-1.csv')\n",
    "\n",
    "data = (\n",
    "    text_file\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "    # 14: ArrDelay, 15: DepDelay\n",
    "    .map(lambda p: (p[14], p[15]))\n",
    "    .filter(lambda line: 'ArrDelay' not in line)\n",
    "    .filter(lambda line: 'NA' not in line)\n",
    "    .map(lambda p: (int(p[0]), int(p[1])))\n",
    "    )\n",
    "\n",
    "len_data = data.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: to_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(rdd):\n",
    "    '''\n",
    "    Transforms the \"ArrDelay\" column into binary labels\n",
    "    that indicate whether a flight arrived late (1) or not (0).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    def trans(x):\n",
    "        if x>=15:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    rdd=rdd.map(lambda p: (trans(p[0]), p[1]))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -4), (0, -5), (1, 11), (0, -3), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "binary_labels = to_binary(data)\n",
    "print(binary_labels.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(binary_labels, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(binary_labels.count(), len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_equal(to_binary(sc.parallelize([(15.0, 120.0)])).first(), (1, 120.0))\n",
    "assert_equal(to_binary(sc.parallelize([(14.9, 450.0)])).first(), (0, 450.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: to_labeled_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_labeled_point(rdd):\n",
    "    '''\n",
    "    Transforms a Spark sequence of tuples into\n",
    "    a sequence containing LabeledPoint values for each row.\n",
    "    \n",
    "    The arrival delay is the label.\n",
    "    The departure delay is the feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    rdd = rdd.map(lambda p: LabeledPoint(p[0],[p[1]]))\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-4.0]), LabeledPoint(0.0, [-5.0]), LabeledPoint(1.0, [11.0]), LabeledPoint(0.0, [-3.0]), LabeledPoint(1.0, [0.0])]\n"
     ]
    }
   ],
   "source": [
    "labeled_point = to_labeled_point(binary_labels)\n",
    "print(labeled_point.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_is_instance(labeled_point, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(labeled_point.count(), len_data)\n",
    "assert_true(all(isinstance(p, LabeledPoint) for p in labeled_point.take(5)))\n",
    "assert_equal([p.label for p in labeled_point.take(5)], [0.0, 0.0, 1.0, 0.0, 1.0])\n",
    "assert_true(all(\n",
    "    isinstance(p.features, pyspark.mllib.linalg.DenseVector)\n",
    "    for p\n",
    "    in labeled_point.take(5)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: fit_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(rdd):\n",
    "    '''\n",
    "    Fits a logistic regression model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An RDD of (label, prediction) pairs.\n",
    "    '''\n",
    "    \n",
    "    lr_model = LogisticRegressionWithLBFGS.train(rdd, iterations=10)\n",
    "    rdd = rdd.map(lambda lp: (lp.label, float(lr_model.predict(lp.features))))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (0.0, 0.0), (1.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "labels_and_preds = fit_and_predict(labeled_point)\n",
    "print(labels_and_preds.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert_is_instance(labels_and_preds, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(labels_and_preds.count(), len_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(rdd):\n",
    "    '''\n",
    "    Computes accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A float.\n",
    "    '''\n",
    "    ac = rdd.map(lambda p: p[0]==p[1])\n",
    "    \n",
    "    accuracy = ac.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7547458269632241\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(labels_and_preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
